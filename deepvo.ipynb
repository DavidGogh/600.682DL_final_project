{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "01Ih8iIZAV7T"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tnrange\n",
    "# from params import par\n",
    "from model import DeepVO\n",
    "from data_helper import get_data_info, SortedRandomBatchSampler, ImageSequenceDataset, get_partition_data_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/davidgogh96/DeepVO/'\n",
    "class Parameters():\n",
    "    def __init__(self):\n",
    "        self.n_processors = 8\n",
    "        # Path\n",
    "        self.data_dir =  DATA_PATH + 'KITTI'\n",
    "        self.image_dir = self.data_dir + '/images/'\n",
    "        self.pose_dir = self.data_dir + '/pose_GT/'\n",
    "\n",
    "        self.train_video = ['00', '01', '02', '05', '08', '10']\n",
    "        self.valid_video = ['04', '06', '07', '09']\n",
    "        self.partition = None  # partition videos in 'train_video' to train / valid dataset  #0.8\n",
    "\n",
    "\n",
    "        # Data Preprocessing\n",
    "        self.resize_mode = 'rescale'  # choice: 'crop' 'rescale' None\n",
    "#         self.img_w = 608   # original size is about 1226\n",
    "#         self.img_h = 184   # original size is about 370\n",
    "        self.img_w = 384   # original size is about 1226\n",
    "        self.img_h = 128   # original size is about 370\n",
    "#         self.img_w = 384   # original size is about 1226\n",
    "#         self.img_h = 128   # original size is about 370\n",
    "        self.img_means = (-0.14968217427134656, -0.12941663107068363, -0.1320610301921484)\n",
    "        self.img_stds = (1, 1, 1)  #(0.309122, 0.315710, 0.3226514)\n",
    "        self.minus_point_5 = True\n",
    "\n",
    "        self.seq_len = (5, 7)\n",
    "        self.sample_times = 3\n",
    "\n",
    "        # Data info path\n",
    "        self.train_data_info_path = 'ndatainfo/train_df_t{}_v{}_p{}_seq{}x{}_sample{}.pickle'.format(''.join(self.train_video), ''.join(self.valid_video), self.partition, self.seq_len[0], self.seq_len[1], self.sample_times)\n",
    "        self.valid_data_info_path = 'ndatainfo/valid_df_t{}_v{}_p{}_seq{}x{}_sample{}.pickle'.format(''.join(self.train_video), ''.join(self.valid_video), self.partition, self.seq_len[0], self.seq_len[1], self.sample_times)\n",
    "\n",
    "\n",
    "        # Model\n",
    "        self.rnn_hidden_size = 1000\n",
    "        self.conv_dropout = (0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.5)\n",
    "        self.rnn_dropout_out = 0.5\n",
    "        self.rnn_dropout_between = 0   # 0: no dropout\n",
    "        self.clip = None\n",
    "        self.batch_norm = True\n",
    "        # Training\n",
    "        self.epochs = 50\n",
    "        self.batch_size = 8\n",
    "        self.pin_mem = False\n",
    "        self.optim = {'opt': 'Adam'}\n",
    "#         {'opt': 'Adagrad', 'lr': 0.0005}\n",
    "                    # Choice:\n",
    "                    # {'opt': 'Adagrad', 'lr': 0.001}\n",
    "                    # {'opt': 'Adam'}\n",
    "                    # {'opt': 'Cosine', 'T': 100 , 'lr': 0.001}\n",
    "\n",
    "        # Pretrain, Resume training\n",
    "        self.pretrained_flownet = DATA_PATH + '/weights/pretrained/flownets_bn_EPE2.459.pth.tar'\n",
    "                                # Choice:\n",
    "                                # None\n",
    "                                # './pretrained/flownets_bn_EPE2.459.pth.tar'  \n",
    "                                # './pretrained/flownets_EPE1.951.pth.tar'\n",
    "        self.resume = False  # resume training\n",
    "        self.resume_t_or_v = '.train'\n",
    "        self.load_model_path = 'nmodels/t{}_v{}_im{}x{}_s{}x{}_b{}_rnn{}_{}.model{}'.format(''.join(self.train_video), ''.join(self.valid_video), self.img_h, self.img_w, self.seq_len[0], self.seq_len[1], self.batch_size, self.rnn_hidden_size, '_'.join([k+str(v) for k, v in self.optim.items()]), self.resume_t_or_v)\n",
    "        self.load_optimizer_path = 'nmodels/t{}_v{}_im{}x{}_s{}x{}_b{}_rnn{}_{}.optimizer{}'.format(''.join(self.train_video), ''.join(self.valid_video), self.img_h, self.img_w, self.seq_len[0], self.seq_len[1], self.batch_size, self.rnn_hidden_size, '_'.join([k+str(v) for k, v in self.optim.items()]), self.resume_t_or_v)\n",
    "\n",
    "        self.record_path = 'nrecords/t{}_v{}_im{}x{}_s{}x{}_b{}_rnn{}_{}.txt'.format(''.join(self.train_video), ''.join(self.valid_video), self.img_h, self.img_w, self.seq_len[0], self.seq_len[1], self.batch_size, self.rnn_hidden_size, '_'.join([k+str(v) for k, v in self.optim.items()]))\n",
    "        self.save_model_path = 'nmodels/t{}_v{}_im{}x{}_s{}x{}_b{}_rnn{}_{}.model'.format(''.join(self.train_video), ''.join(self.valid_video), self.img_h, self.img_w, self.seq_len[0], self.seq_len[1], self.batch_size, self.rnn_hidden_size, '_'.join([k+str(v) for k, v in self.optim.items()]))\n",
    "        self.save_optimzer_path = 'nmodels/t{}_v{}_im{}x{}_s{}x{}_b{}_rnn{}_{}.optimizer'.format(''.join(self.train_video), ''.join(self.valid_video), self.img_h, self.img_w, self.seq_len[0], self.seq_len[1], self.batch_size, self.rnn_hidden_size, '_'.join([k+str(v) for k, v in self.optim.items()]))\n",
    "\n",
    "\n",
    "        if not os.path.isdir(os.path.dirname(self.record_path)):\n",
    "            os.makedirs(os.path.dirname(self.record_path))\n",
    "        if not os.path.isdir(os.path.dirname(self.save_model_path)):\n",
    "            os.makedirs(os.path.dirname(self.save_model_path))\n",
    "        if not os.path.isdir(os.path.dirname(self.save_optimzer_path)):\n",
    "            os.makedirs(os.path.dirname(self.save_optimzer_path))\n",
    "        if not os.path.isdir(os.path.dirname(self.train_data_info_path)):\n",
    "            os.makedirs(os.path.dirname(self.train_data_info_path))\n",
    "\n",
    "par = Parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from params import par\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import kaiming_normal_, orthogonal_\n",
    "import numpy as np\n",
    "def conv(batchNorm, in_planes, out_planes, kernel_size=3, stride=1, dropout=0):\n",
    "    if batchNorm:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=False),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(dropout)#, inplace=True)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=(kernel_size-1)//2, bias=True),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Dropout(dropout)#, inplace=True)\n",
    "        )\n",
    "class DeepVO(nn.Module):\n",
    "    def __init__(self, imsize1, imsize2, batchNorm=True):\n",
    "        super(DeepVO,self).__init__()\n",
    "        # CNN\n",
    "        self.batchNorm = batchNorm\n",
    "        self.clip = par.clip\n",
    "        self.conv1   = conv(self.batchNorm,   6,   64, kernel_size=7, stride=2, dropout=par.conv_dropout[0])\n",
    "        self.conv2   = conv(self.batchNorm,  64,  128, kernel_size=5, stride=2, dropout=par.conv_dropout[1])\n",
    "        self.conv3   = conv(self.batchNorm, 128,  256, kernel_size=5, stride=2, dropout=par.conv_dropout[2])\n",
    "        self.conv3_1 = conv(self.batchNorm, 256,  256, kernel_size=3, stride=1, dropout=par.conv_dropout[3])\n",
    "        self.conv4   = conv(self.batchNorm, 256,  512, kernel_size=3, stride=2, dropout=par.conv_dropout[4])\n",
    "        self.conv4_1 = conv(self.batchNorm, 512,  512, kernel_size=3, stride=1, dropout=par.conv_dropout[5])\n",
    "        self.conv5   = conv(self.batchNorm, 512,  512, kernel_size=3, stride=2, dropout=par.conv_dropout[6])\n",
    "        self.conv5_1 = conv(self.batchNorm, 512,  512, kernel_size=3, stride=1, dropout=par.conv_dropout[7])\n",
    "        self.conv6   = conv(self.batchNorm, 512, 1024, kernel_size=3, stride=2, dropout=par.conv_dropout[8])\n",
    "        # Comput the shape based on diff image size\n",
    "        __tmp = Variable(torch.zeros(1, 6, imsize1, imsize2))\n",
    "        __tmp = self.encode_image(__tmp)\n",
    "\n",
    "        # RNN\n",
    "        self.rnn = nn.LSTM(\n",
    "                    input_size=int(np.prod(__tmp.size())), \n",
    "                    hidden_size=par.rnn_hidden_size, \n",
    "                    num_layers=2, \n",
    "                    dropout=par.rnn_dropout_between, \n",
    "                    batch_first=True)\n",
    "        self.rnn_drop_out = nn.Dropout(par.rnn_dropout_out)\n",
    "        self.linear = nn.Linear(in_features=par.rnn_hidden_size, out_features=6)\n",
    "\n",
    "        # Initilization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Linear):\n",
    "                kaiming_normal_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                # layer 1\n",
    "                kaiming_normal_(m.weight_ih_l0)  #orthogonal_(m.weight_ih_l0)\n",
    "                kaiming_normal_(m.weight_hh_l0)\n",
    "                m.bias_ih_l0.data.zero_()\n",
    "                m.bias_hh_l0.data.zero_()\n",
    "                # Set forget gate bias to 1 (remember)\n",
    "                n = m.bias_hh_l0.size(0)\n",
    "                start, end = n//4, n//2\n",
    "                m.bias_hh_l0.data[start:end].fill_(1.)\n",
    "\n",
    "                # layer 2\n",
    "                kaiming_normal_(m.weight_ih_l1)  #orthogonal_(m.weight_ih_l1)\n",
    "                kaiming_normal_(m.weight_hh_l1)\n",
    "                m.bias_ih_l1.data.zero_()\n",
    "                m.bias_hh_l1.data.zero_()\n",
    "                n = m.bias_hh_l1.size(0)\n",
    "                start, end = n//4, n//2\n",
    "                m.bias_hh_l1.data[start:end].fill_(1.)\n",
    "\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        # x: (batch, seq_len, channel, width, height)\n",
    "        # stack_image\n",
    "        x = torch.cat(( x[:, :-1], x[:, 1:]), dim=2)\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "        # CNN\n",
    "        x = x.view(batch_size*seq_len, x.size(2), x.size(3), x.size(4))\n",
    "        x = self.encode_image(x)\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "\n",
    "\n",
    "        # RNN\n",
    "        out, hc = self.rnn(x)\n",
    "        out = self.rnn_drop_out(out)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "        \n",
    "\n",
    "    def encode_image(self, x):\n",
    "        out_conv2 = self.conv2(self.conv1(x))\n",
    "        out_conv3 = self.conv3_1(self.conv3(out_conv2))\n",
    "        out_conv4 = self.conv4_1(self.conv4(out_conv3))\n",
    "        out_conv5 = self.conv5_1(self.conv5(out_conv4))\n",
    "        out_conv6 = self.conv6(out_conv5)\n",
    "#         print(self.conv1(x).shape)\n",
    "#         print(out_conv2.shape)\n",
    "#         print(out_conv3.shape)\n",
    "#         print(out_conv4.shape)\n",
    "#         print(out_conv5.shape)\n",
    "#         print(out_conv6.shape)\n",
    "        return out_conv6\n",
    "\n",
    "    def weight_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'weight' in name]\n",
    "\n",
    "    def bias_parameters(self):\n",
    "        return [param for name, param in self.named_parameters() if 'bias' in name]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_velocity(y):\n",
    "        return (y[:, :-1, :] - y[:, 1:, :]).norm(2, 2)\n",
    "    \n",
    "    def get_loss(self, x, y):\n",
    "        predicted = self.forward(x)\n",
    "        y = y[:, 1:, :]  # (batch, seq, dim_pose)\n",
    "#         print(predicted.shape, y.shape)\n",
    "        # Weighted MSE Loss\n",
    "        angle_loss = torch.nn.functional.mse_loss(predicted[:,:,:3], y[:,:,:3])\n",
    "        translation_loss = torch.nn.functional.mse_loss(predicted[:,:,3:], y[:,:,3:])\n",
    "        loss = (100 * angle_loss + translation_loss)\n",
    "        trans_v_loss = torch.nn.functional.mse_loss(self.get_velocity(predicted[:, :, 3:]), self.get_velocity(y[:, :, 3:]))\n",
    "        angle_v_loss = torch.nn.functional.mse_loss(self.get_velocity(predicted[:, :, :3]), self.get_velocity(y[:, :, :3]))\n",
    "        loss += trans_v_loss + angle_v_loss\n",
    "        return loss\n",
    "\n",
    "    def step(self, x, y, optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss = self.get_loss(x, y)\n",
    "        loss.backward()\n",
    "        if self.clip != None:\n",
    "            torch.nn.utils.clip_grad_norm(self.rnn.parameters(), self.clip)\n",
    "        optimizer.step()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data info from datainfo/train_df_t000102050810_v040607_pNone_seq5x7_sample3.pickle\n",
      "Number of samples in training dataset:  11031\n",
      "Number of samples in validation dataset:  1485\n",
      "==================================================\n",
      "CUDA used.\n"
     ]
    }
   ],
   "source": [
    "# Write all hyperparameters to record_path\n",
    "mode = 'a' if par.resume else 'w'\n",
    "with open(par.record_path, mode) as f:\n",
    "    f.write('\\n'+'='*50 + '\\n')\n",
    "    f.write('\\n'.join(\"%s: %s\" % item for item in vars(par).items()))\n",
    "    f.write('\\n'+'='*50 + '\\n')\n",
    "\n",
    "# Prepare Data\n",
    "if os.path.isfile(par.train_data_info_path) and os.path.isfile(par.valid_data_info_path):\n",
    "    print('Load data info from {}'.format(par.train_data_info_path))\n",
    "    train_df = pd.read_pickle(par.train_data_info_path)\n",
    "    valid_df = pd.read_pickle(par.valid_data_info_path)\n",
    "else:\n",
    "    print('Create new data info')\n",
    "    if par.partition != None:\n",
    "        partition = par.partition\n",
    "        train_df, valid_df = get_partition_data_info(partition, par.train_video, par.seq_len, overlap=1, sample_times=par.sample_times, shuffle=True, sort=True)\n",
    "    else:\n",
    "        train_df = get_data_info(folder_list=par.train_video, seq_len_range=par.seq_len, overlap=1, sample_times=par.sample_times)\t\n",
    "        valid_df = get_data_info(folder_list=par.valid_video, seq_len_range=par.seq_len, overlap=1, sample_times=par.sample_times)\n",
    "    # save the data info\n",
    "    train_df.to_pickle(par.train_data_info_path)\n",
    "    valid_df.to_pickle(par.valid_data_info_path)\n",
    "\n",
    "train_sampler = SortedRandomBatchSampler(train_df, par.batch_size, drop_last=True)\n",
    "train_dataset = ImageSequenceDataset(train_df, par.resize_mode, (par.img_w, par.img_h), par.img_means, par.img_stds, par.minus_point_5)\n",
    "train_dl = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=par.n_processors, pin_memory=par.pin_mem)\n",
    "\n",
    "valid_sampler = SortedRandomBatchSampler(valid_df, par.batch_size, drop_last=True)\n",
    "valid_dataset = ImageSequenceDataset(valid_df, par.resize_mode, (par.img_w, par.img_h), par.img_means, par.img_stds, par.minus_point_5)\n",
    "valid_dl = DataLoader(valid_dataset, batch_sampler=valid_sampler, num_workers=par.n_processors, pin_memory=par.pin_mem)\n",
    "\n",
    "print('Number of samples in training dataset: ', len(train_df.index))\n",
    "print('Number of samples in validation dataset: ', len(valid_df.index))\n",
    "print('='*50)\n",
    "\n",
    "\n",
    "# Model\n",
    "M_deepvo = DeepVO(par.img_h, par.img_w, par.batch_norm)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('CUDA used.')\n",
    "    M_deepvo = M_deepvo.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, t_x, t_y = next(iter(train_dl))\n",
    "t_x = t_x.cuda(non_blocking=par.pin_mem)\n",
    "t_y = t_y.cuda(non_blocking=par.pin_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = M_deepvo.forward(t_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 6, 6]) torch.Size([8, 6, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_deepvo.get_loss(t_x, t_y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = t_y[:, 1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_velocity(y):\n",
    "    return (y[:, :-1, :] - y[:, 1:, :]).norm(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4295, device='cuda:0', grad_fn=<MseLossBackward>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.functional.mse_loss(p[:, :, 3:], y[:, :, 3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mzSWELOAAV-P",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data info from ndatainfo/train_df_t000102050810_v04060709_pNone_seq5x7_sample3.pickle\n",
      "Number of samples in training dataset:  10996\n",
      "Number of samples in validation dataset:  2435\n",
      "==================================================\n",
      "CUDA used.\n",
      "Load FlowNet pretrained model\n",
      "Record loss in:  nrecords/t000102050810_v04060709_im128x384_s5x7_b8_rnn1000_optAdam.txt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae4a792af5943a2a4cd6be35437060c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f49f4e2014044cc84181534a4817100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=1373, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train take 1126.5 sec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f1f4bae725489a932aeda88ac9a7e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Validating', max=304, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid take 198.8 sec\n",
      "Epoch 1\n",
      "train loss mean: 18.688979657294972, std: 33.87\n",
      "valid loss mean: 20.559200522832963, std: 30.89\n",
      "\n",
      "Save model at ep 1, mean of valid loss: 20.559200522832963\n",
      "Save model at ep 1, mean of train loss: 18.688979657294972\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c38d95cac54ed6a354f1fd7cf4ee94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training', max=1373, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write all hyperparameters to record_path\n",
    "mode = 'a' if par.resume else 'w'\n",
    "with open(par.record_path, mode) as f:\n",
    "    f.write('\\n'+'='*50 + '\\n')\n",
    "    f.write('\\n'.join(\"%s: %s\" % item for item in vars(par).items()))\n",
    "    f.write('\\n'+'='*50 + '\\n')\n",
    "\n",
    "# Prepare Data\n",
    "if os.path.isfile(par.train_data_info_path) and os.path.isfile(par.valid_data_info_path):\n",
    "    print('Load data info from {}'.format(par.train_data_info_path))\n",
    "    train_df = pd.read_pickle(par.train_data_info_path)\n",
    "    valid_df = pd.read_pickle(par.valid_data_info_path)\n",
    "else:\n",
    "    print('Create new data info')\n",
    "    if par.partition != None:\n",
    "        partition = par.partition\n",
    "        train_df, valid_df = get_partition_data_info(partition, par.train_video, par.seq_len, overlap=1, sample_times=par.sample_times, shuffle=True, sort=True)\n",
    "    else:\n",
    "        train_df = get_data_info(folder_list=par.train_video, seq_len_range=par.seq_len, overlap=1, sample_times=par.sample_times)\t\n",
    "        valid_df = get_data_info(folder_list=par.valid_video, seq_len_range=par.seq_len, overlap=1, sample_times=par.sample_times)\n",
    "    # save the data info\n",
    "    train_df.to_pickle(par.train_data_info_path)\n",
    "    valid_df.to_pickle(par.valid_data_info_path)\n",
    "\n",
    "train_sampler = SortedRandomBatchSampler(train_df, par.batch_size, drop_last=True)\n",
    "train_dataset = ImageSequenceDataset(train_df, par.resize_mode, (par.img_w, par.img_h), par.img_means, par.img_stds, par.minus_point_5)\n",
    "train_dl = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=par.n_processors, pin_memory=par.pin_mem)\n",
    "\n",
    "valid_sampler = SortedRandomBatchSampler(valid_df, par.batch_size, drop_last=True)\n",
    "valid_dataset = ImageSequenceDataset(valid_df, par.resize_mode, (par.img_w, par.img_h), par.img_means, par.img_stds, par.minus_point_5)\n",
    "valid_dl = DataLoader(valid_dataset, batch_sampler=valid_sampler, num_workers=par.n_processors, pin_memory=par.pin_mem)\n",
    "\n",
    "print('Number of samples in training dataset: ', len(train_df.index))\n",
    "print('Number of samples in validation dataset: ', len(valid_df.index))\n",
    "print('='*50)\n",
    "\n",
    "\n",
    "# Model\n",
    "M_deepvo = DeepVO(par.img_h, par.img_w, par.batch_norm)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('CUDA used.')\n",
    "    M_deepvo = M_deepvo.cuda()\n",
    "\n",
    "\n",
    "# Load FlowNet weights pretrained with FlyingChairs\n",
    "# NOTE: the pretrained model assumes image rgb values in range [-0.5, 0.5]\n",
    "if par.pretrained_flownet and not par.resume:\n",
    "    if use_cuda:\n",
    "        pretrained_w = torch.load(par.pretrained_flownet)\n",
    "    else:\n",
    "        pretrained_w = torch.load(par.pretrained_flownet_flownet, map_location='cpu')\n",
    "    print('Load FlowNet pretrained model')\n",
    "    # Use only conv-layer-part of FlowNet as CNN for DeepVO\n",
    "    model_dict = M_deepvo.state_dict()\n",
    "    update_dict = {k: v for k, v in pretrained_w['state_dict'].items() if k in model_dict}\n",
    "    model_dict.update(update_dict)\n",
    "    M_deepvo.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "# Create optimizer\n",
    "if par.optim['opt'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(M_deepvo.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "elif par.optim['opt'] == 'Adagrad':\n",
    "    optimizer = torch.optim.Adagrad(M_deepvo.parameters(), lr=par.optim['lr'])\n",
    "elif par.optim['opt'] == 'Cosine':\n",
    "    optimizer = torch.optim.SGD(M_deepvo.parameters(), lr=par.optim['lr'])\n",
    "    T_iter = par.optim['T']*len(train_dl)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_iter, eta_min=0, last_epoch=-1)\n",
    "\n",
    "# Load trained DeepVO model and optimizer\n",
    "if par.resume:\n",
    "    M_deepvo.load_state_dict(torch.load(par.load_model_path))\n",
    "    optimizer.load_state_dict(torch.load(par.load_optimizer_path))\n",
    "    print('Load model from: ', par.load_model_path)\n",
    "    print('Load optimizer from: ', par.load_optimizer_path)\n",
    "\n",
    "\n",
    "# Train\n",
    "print('Record loss in: ', par.record_path)\n",
    "min_loss_t = 1e10\n",
    "min_loss_v = 1e10\n",
    "M_deepvo.train()\n",
    "t_loss_list = []\n",
    "v_loss_list = []\n",
    "for ep in tnrange(par.epochs):\n",
    "    st_t = time.time()\n",
    "#     print('='*50)\n",
    "    # Train\n",
    "    M_deepvo.train()\n",
    "    loss_mean = 0\n",
    "    \n",
    "    i = 1\n",
    "    progress_bar = tqdm(train_dl, desc='Training')\n",
    "    for batch_idx, (_, t_x, t_y) in enumerate(progress_bar):\n",
    "#     for _, t_x, t_y in train_dl:\n",
    "        if use_cuda:\n",
    "            t_x = t_x.cuda(non_blocking=par.pin_mem)\n",
    "            t_y = t_y.cuda(non_blocking=par.pin_mem)\n",
    "        ls = M_deepvo.step(t_x, t_y, optimizer).data.cpu().numpy()\n",
    "        t_loss_list.append(float(ls))\n",
    "        loss_mean += float(ls)\n",
    "        if par.optim == 'Cosine':\n",
    "            lr_scheduler.step()\n",
    "        progress_bar.set_description(\n",
    "        'Epoch: {} loss: {:.4f}'.format(\n",
    "            ep, loss_mean / (batch_idx + 1)))\n",
    "    print('Train take {:.1f} sec'.format(time.time()-st_t))\n",
    "    loss_mean /= len(train_dl)\n",
    "\n",
    "    # Validation\n",
    "    st_t = time.time()\n",
    "    M_deepvo.eval()\n",
    "    loss_mean_valid = 0\n",
    "    \n",
    "    progress_bar = tqdm(valid_dl, desc='Validating')\n",
    "    for batch_idx, (_, v_x, v_y) in enumerate(progress_bar):\n",
    "        if use_cuda:\n",
    "            v_x = v_x.cuda(non_blocking=par.pin_mem)\n",
    "            v_y = v_y.cuda(non_blocking=par.pin_mem)\n",
    "        v_ls = M_deepvo.get_loss(v_x, v_y).data.cpu().numpy()\n",
    "        v_loss_list.append(float(v_ls))\n",
    "        loss_mean_valid += float(v_ls)\n",
    "        progress_bar.set_description(\n",
    "        'Epoch: {} loss: {:.4f}'.format(\n",
    "            ep, loss_mean_valid / (batch_idx + 1)))\n",
    "    print('Valid take {:.1f} sec'.format(time.time()-st_t))\n",
    "    loss_mean_valid /= len(valid_dl)\n",
    "\n",
    "\n",
    "    f = open(par.record_path, 'a')\n",
    "    f.write('Epoch {}\\ntrain loss mean: {}, std: {:.2f}\\nvalid loss mean: {}, std: {:.2f}\\n'.format(ep+1, loss_mean, np.std(t_loss_list), loss_mean_valid, np.std(v_loss_list)))\n",
    "    print('Epoch {}\\ntrain loss mean: {}, std: {:.2f}\\nvalid loss mean: {}, std: {:.2f}\\n'.format(ep+1, loss_mean, np.std(t_loss_list), loss_mean_valid, np.std(v_loss_list)))\n",
    "\n",
    "    # Save model\n",
    "    # save if the valid loss decrease\n",
    "    check_interval = 1\n",
    "    if loss_mean_valid < min_loss_v and ep % check_interval == 0:\n",
    "        min_loss_v = loss_mean_valid\n",
    "        print('Save model at ep {}, mean of valid loss: {}'.format(ep+1, loss_mean_valid))  # use 4.6 sec \n",
    "        torch.save(M_deepvo.state_dict(), par.save_model_path+'.valid')\n",
    "        torch.save(optimizer.state_dict(), par.save_optimzer_path+'.valid')\n",
    "    # save if the training loss decrease\n",
    "    check_interval = 1\n",
    "    if loss_mean < min_loss_t and ep % check_interval == 0:\n",
    "        min_loss_t = loss_mean\n",
    "        print('Save model at ep {}, mean of train loss: {}'.format(ep+1, loss_mean))\n",
    "        torch.save(M_deepvo.state_dict(), par.save_model_path+'.train')\n",
    "        torch.save(optimizer.state_dict(), par.save_optimzer_path+'.train')\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "# videos_to_test = ['04', '05', '07', '10', '09']\n",
    "videos_to_test = ['00', '02', '08', '09']\n",
    "videos_to_test += ['01', '04', '05', '06', '07', '10']\n",
    "\n",
    "# Path\n",
    "# load_model_path = DATA_PATH + 'weights/models/t000102050809_v04060710_im184x608_s5x7_b8_rnn1000_optAdagrad_lr0.0005.model.train'\n",
    "load_model_path = par.load_model_path   #choose the model you want to load\n",
    "save_dir = 'result/'  # directory to save prediction answer\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "\n",
    "# Load model\n",
    "M_deepvo = DeepVO(par.img_h, par.img_w, par.batch_norm)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    M_deepvo = M_deepvo.cuda()\n",
    "    M_deepvo.load_state_dict(torch.load(load_model_path))\n",
    "else:\n",
    "    M_deepvo.load_state_dict(torch.load(load_model_path, map_location={'cuda:0': 'cpu'}))\n",
    "print('Load model from: ', load_model_path)\n",
    "\n",
    "\n",
    "\n",
    "# Data\n",
    "n_workers = 1\n",
    "seq_len = int((par.seq_len[0]+par.seq_len[1])/2)\n",
    "overlap = seq_len - 1\n",
    "print('seq_len = {},  overlap = {}'.format(seq_len, overlap))\n",
    "batch_size = par.batch_size\n",
    "\n",
    "for test_video in videos_to_test:\n",
    "    df = get_data_info(folder_list=[test_video], seq_len_range=[seq_len, seq_len], overlap=overlap, sample_times=1, shuffle=False, sort=False)\n",
    "    df = df.loc[df.seq_len == seq_len]  # drop last\n",
    "    dataset = ImageSequenceDataset(df, par.resize_mode, (par.img_w, par.img_h), par.img_means, par.img_stds, par.minus_point_5)\n",
    "    df.to_csv('test_df.csv')\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=n_workers)\n",
    "    gt_pose = np.load('{}{}.npy'.format(par.pose_dir, test_video))  # (n_images, 6)\n",
    "\n",
    "    # Predict\n",
    "    M_deepvo.eval()\n",
    "    has_predict = False\n",
    "    answer = [[0.0]*6, ]\n",
    "    st_t = time.time()\n",
    "    n_batch = len(dataloader)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        print('{} / {}'.format(i, n_batch), end='\\r', flush=True)\n",
    "        _, x, y = batch\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "        batch_predict_pose = M_deepvo.forward(x)\n",
    "\n",
    "        # Record answer\n",
    "        batch_predict_pose = batch_predict_pose.data.cpu().numpy()\n",
    "        if i == 0:\n",
    "            for pose in batch_predict_pose[0]:\n",
    "                # use all predicted pose in the first prediction\n",
    "                for i in range(len(pose)):\n",
    "                    # Convert predicted relative pose to absolute pose by adding last pose\n",
    "                    pose[i] += answer[-1][i]\n",
    "                answer.append(pose.tolist())\n",
    "            batch_predict_pose = batch_predict_pose[1:]\n",
    "\n",
    "        for predict_pose_seq in batch_predict_pose:\n",
    "        # use only last predicted pose in the following prediction\n",
    "            last_pose = predict_pose_seq[-1]\n",
    "            for i in range(len(last_pose)):\n",
    "                last_pose[i] += answer[-1][i]\n",
    "            answer.append(last_pose.tolist())\n",
    "    print('len(answer): ', len(answer))\n",
    "    print('expect len: ', len(glob.glob('{}{}/*.png'.format(par.image_dir, test_video))))\n",
    "    print('Predict use {} sec'.format(time.time() - st_t))\n",
    "\n",
    "\n",
    "    # Save answer\n",
    "    with open('{}/out_{}.txt'.format(save_dir, test_video), 'w') as f:\n",
    "        for pose in answer:\n",
    "            if type(pose) == list:\n",
    "                f.write(', '.join([str(p) for p in pose]))\n",
    "            else:\n",
    "                f.write(str(pose))\n",
    "            f.write('\\n')\n",
    "\n",
    "\n",
    "    # Calculate loss\n",
    "    gt_pose = np.load('{}{}.npy'.format(par.pose_dir, test_video))  # (n_images, 6)\n",
    "    loss = 0\n",
    "    for t in range(len(gt_pose)):\n",
    "        angle_loss = np.sum((answer[t][:3] - gt_pose[t,:3]) ** 2)\n",
    "        translation_loss = np.sum((answer[t][3:] - gt_pose[t,3:]) ** 2)\n",
    "        loss = (100 * angle_loss + translation_loss)\n",
    "    loss /= len(gt_pose)\n",
    "    print('Loss = ', loss)\n",
    "    print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ion()\n",
    "\n",
    "pose_GT_dir = par.pose_dir  #'KITTI/pose_GT/'\n",
    "predicted_result_dir = './result/'\n",
    "gradient_color = False\n",
    "\n",
    "def plot_route(gt, out, c_gt='g', c_out='r'):\n",
    "    x_idx = 3\n",
    "    y_idx = 5\n",
    "    x = [v for v in gt[:, x_idx]]\n",
    "    y = [v for v in gt[:, y_idx]]\n",
    "    plt.plot(x, y, color=c_gt, label='Ground Truth')\n",
    "    #plt.scatter(x, y, color='b')\n",
    "\n",
    "    x = [v for v in out[:, x_idx]]\n",
    "    y = [v for v in out[:, y_idx]]\n",
    "    plt.plot(x, y, color=c_out, label='DeepVO')\n",
    "    #plt.scatter(x, y, color='b')\n",
    "    plt.gca().set_aspect('equal', adjustable='datalim')\n",
    "\n",
    "\n",
    "# Load in GT and predicted pose\n",
    "video_list = ['00', '01', '02', '05', '08', '10']\n",
    "video_list += ['04', '06', '07', '09']\n",
    "\n",
    "\n",
    "for video in video_list:\n",
    "    print('='*50)\n",
    "    print('Video {}'.format(video))\n",
    "\n",
    "    GT_pose_path = '{}{}.npy'.format(pose_GT_dir, video)\n",
    "    gt = np.load(GT_pose_path)\n",
    "    pose_result_path = '{}out_{}.txt'.format(predicted_result_dir, video)\n",
    "    with open(pose_result_path) as f_out:\n",
    "        out = [l.split('\\n')[0] for l in f_out.readlines()]\n",
    "        for i, line in enumerate(out):\n",
    "            out[i] = [float(v) for v in line.split(',')]\n",
    "        out = np.array(out)\n",
    "        mse_rotate = 100 * np.mean((out[:, :3] - gt[:, :3])**2)\n",
    "        mse_translate = np.mean((out[:, 3:] - gt[:, 3:])**2)\n",
    "        print('mse_rotate: ', mse_rotate)\n",
    "        print('mse_translate: ', mse_translate)\n",
    "        plt.clf()\n",
    "        plt.title('Video {} errors'.format(video))\n",
    "        plt.plot(np.linalg.norm(out[:, :3] - gt[:, :3], axis=1), label='rotation err')\n",
    "        plt.plot(np.linalg.norm(out[:, 3:] - gt[:, 3:], axis=1), label='translation err')\n",
    "        plt.legend()\n",
    "        save_name = '{}route_{}_err.png'.format(predicted_result_dir, video)\n",
    "        plt.savefig(save_name)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "    if gradient_color:\n",
    "        # plot gradient color\n",
    "        step = 200\n",
    "        plt.clf()\n",
    "        plt.scatter([gt[0][3]], [gt[0][5]], label='sequence start', marker='s', color='k')\n",
    "        for st in range(0, len(out), step):\n",
    "            end = st + step\n",
    "            g = max(0.2, st/len(out))\n",
    "            c_gt = (0, g, 0)\n",
    "            c_out = (1, g, 0)\n",
    "            plot_route(gt[st:end], out[st:end], c_gt, c_out)\n",
    "            if st == 0:\n",
    "                plt.legend()\n",
    "            plt.title('Video {}'.format(video))\n",
    "            save_name = '{}route_{}_gradient.png'.format(predicted_result_dir, video)\n",
    "        plt.savefig(save_name)\n",
    "    else:\n",
    "        # plot one color\n",
    "        plt.clf()\n",
    "        plt.scatter([gt[0][3]], [gt[0][5]], label='sequence start', marker='s', color='k')\n",
    "        plot_route(gt, out, 'r', 'b')\n",
    "        plt.legend()\n",
    "        plt.title('Video {}'.format(video))\n",
    "        save_name = '{}route_{}.png'.format(predicted_result_dir, video)\n",
    "        plt.savefig(save_name)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deepvo.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
